#Notes for Elastic search cluster monitoring
#This elastic cluster is of the version 7.6
#It is running on Mesos DCOS D2IQ

#Get the state of the cluster
GET /_cluster/health?pretty=true

#Get the list of nodes,heap,ram and load on cpu
GET /_cat/nodes?v&pretty

#Get the shard assignment for all the indices
GET /_cat/shards?v&pretty
GET /_cat/shards?v

#Get the shard assignment for a particular index in my case it is kafka index from the kafka cluster
GET /_cat/shards/kafka-2020.05.12?v&pretty

#Get info about a particular index
GET /_cat/kafka-2020.05.12

#Use cluster allocation explain API to find any issues, if it returns 400 then there is no allocation
GET _cluster/allocation/explain

GET /_nodes/stats?pretty

#Backup and restore of an index is done via the snapshot API,
#create local EFS filesystem for both the data nodes and then take the snapshot of the index
#whenever needed restore them as well. Set the path.repo value in the elasticsearch.yml file and restart elastic server. 
#If you dont set the directory it will not be whitelisted.Give a name to your repo

PUT _snapshot/my_repo
{
  "type":"fs",
  "settings":{
    "location": "/home/centos/snapshots"
  }
}

#By default snapshot api calls are asynchronous, so if you want to be a synchronous call then add the wait completion
PUT _snapshot/my_repo/kafka-2020.05.12?wait_for_completion=true
{
  "indices": "kafka-2020.05.12"
}

#delete the index
DELETE kafka-2020.05.12

#rename them as they are restored
POST _snapshot/my_repo/kafka-2020.05.12/_restore
{
  "indices": "kafka-*",
  "rename_pattern": "index_(.+)",
  "rename_replacement": "$1_restored"
}


#Setup Hot warm arichtecture depending on the old/recent data
#Hot/warm architecture has become a popular data storage solution for companies with high data retention goals, 
#but they don't have the budget to buy massive amounts of high-speed storage. 
#With hot/warm architecture, we can store our most recent (and typically most relevant) data on faster, 
#more expensive storage and then re-allocate that data to cheaper nodes when it becomes less important. 
#Create NODE ATTRIBUTES as HOT and WARM in your elasticsearch.yml as part of the initial setup.

#Command to get the node attributes

GET /_cat/nodeattrs?v

PUT kafka-2020.05.12/_settings
{
  "index.routing.allocation.require.temp":"warm"
}

#Create an index by giving the number of replicas and shards
PUT sample_1
{
 "settings": 
 {
    "number_of_replicas" : 0,
    "number_of_shards" : 2
 }
}

#Allocate the Shards of an Index to Specific Nodes Based on a Given Set of Requirements
#These commands will only be executed if the cluster state will be in GREEN otherwise it wont be executed
PUT kafka-2020.05.12/_settings
{
  "index.routing.allocation.exclude._name":"data-2"
}

#To revert the above settings use the below command
PUT kafka-2020.05.12/_settings
{
  "index.routing.allocation.exclude._name": null
}

#Routing allocation can be done at the cluster level also.
#Cluster settings also can be changed , they fall under transient (cannot withstand restarts) and persistent

PUT _cluster/settings
{
"transient":{
  "cluster.routing.allocation.exclude._name": "data-2"
  }
}
#For mature production clusters that require maximum uptime and fault tolerance, 
#you should segment your cluster into zones (racks, host machines, data centers, etc.). 
#You can also make shard allocation more intelligent by using node attributes to inform Elasticsearch that 
#you have multiple zones. Instead of being allocated to the same zone, primary and replica shards are spread 
#out to improve fault tolerance in the event that an entire zone fails.
#primary shards are kept in one zone and replica shards are placed in another zone
#if one zone fails we should ensure it doesnt throttle the other zone, so mention a shard should only be 
#in only a limited set of zones so that when a zone is down it is not allowed to replicate in any other zone.
#Configure shard allocation awareness
#Configure forced shard allocation awareness
#Determine which awareness method to use and why

#cluster.routing.allocation.awareness.attributes:"zone"
#cluster.routing.allocation.awareness.force.zone.values:"1,2"



